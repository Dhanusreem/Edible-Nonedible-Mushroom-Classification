# -*- coding: utf-8 -*-
"""binaryclassificationsubmission.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p7UdbYsfffSjd2H-aWYoHR7Q1RmGm5-B

Importing and Reading the csv files
"""

#importing the neccesary library
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

#reading the file
df = pd.read_csv('mushroom_cleaned.csv')
df.head(10)

"""Data pre-processing"""

import pandas as pd

# Assuming 'df' is your DataFrame
columns_to_drop = ["season", "stem-color", "stem-width", "stem-height", "gill-color", "gill-attachment","cap-diameter"]
df.drop(columns_to_drop, axis=1, inplace=True)

# Print the updated DataFrame to verify the columns are dropped
print(df)

df.info() # type of the data

df.isna().sum() #Finding the total null values

df2=df.dropna() # droping the null values

df2.isna().sum()

df.duplicated() #Finding the duplicates in the data

df.drop_duplicates() # droping the duplicate values

(df.isna().sum()/len(df))*100 # finding the percentage of the null values

"""Data Visualization

"""

import pandas as pd
import matplotlib.pyplot as plt

spam_counts = df['class'].value_counts()

# Plot a bar graph
plt.bar(spam_counts.index, spam_counts.values, color=['blue', 'green'])
plt.xlabel('In-edible (0) vs edible(1)')
plt.xticks(rotation=90)
plt.ylabel('Number of Instances')
plt.title('Distribution of edible vs In-edible Instances')

"""Hyperparameter Tuning"""

X = df.iloc[:, :-1].values
y = df.iloc[:, -1].values

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30)
# 30 % testing  data and 70% training data

"""Accuracy"""

from sklearn.metrics import accuracy_score
import pandas as pd

# Assuming df is your DataFrame with columns 'cap_shape' and 'class'
# Generate sample data for demonstration
data = {'cap_shape': ['convex', 'bell', 'convex', 'flat', 'convex'],
        'class': [1, 0, 1, 0, 1]}  # Binary class (0/1)

df = pd.DataFrame(data)

# Assuming y_true is the true labels and y_pred is the predicted labels
y_true = df['class']
y_pred = df['cap_shape'].apply(lambda x: 1 if x == 'convex' else 0)  # Binary classification based on 'cap_shape'

# Calculate accuracy
accuracy = accuracy_score(y_true, y_pred)

print(f'Accuracy: {accuracy:.2f}')

"""Correlation Matrix"""

# Calculate correlation coefficients between numeric features and target variable
correlation_matrix = numeric_df.corr()
correlation_with_label = correlation_matrix[target_variable].sort_values(ascending=False)
print("Correlation with target variable (class):")
print(correlation_with_label)

print(df.columns)

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming 'df' is your DataFrame
encoded_df = pd.get_dummies(df, drop_first=True)  # One-hot encode categorical variables
correlation_matrix = encoded_df.corr()

# Plot heatmap
plt.figure(figsize=(8,6))  # Adjust the figure size if needed
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix Heatmap')
plt.show()

"""Scaling"""

x = df.iloc[::,:-1]
y = df.iloc[::,-1:]

from sklearn.preprocessing import MinMaxScaler
import pandas as pd

# Assuming 'X' is your DataFrame
minmax = MinMaxScaler()
minmax_data = minmax.fit_transform(X)

# Create a new DataFrame with the scaled data and the same column names
minmax_df = pd.DataFrame(minmax_data, columns=X.columns)
minmax_df

"""LOGISTIC REGRESSION"""

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import boxcox

def sigmoid(z):
    """Sigmoid activation function."""
    return 1 / (1 + np.exp(-z))

def compute_cost(X, y, theta):
    """Compute the cost (logistic loss) for logistic regression."""
    m = len(y)
    h = sigmoid(np.dot(X, theta))
    cost = (-1 / m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))
    return cost

def compute_gradient(X, y, theta):
    """Compute the gradient of the cost function for logistic regression."""
    m = len(y)
    h = sigmoid(np.dot(X, theta))
    gradient = (1 / m) * np.dot(X.T, (h - y))
    return gradient

def update_parameters(theta, gradient, learning_rate):
    """Update the parameters (weights and bias) using gradient descent."""
    theta -= learning_rate * gradient
    return theta

def predict(X, theta):
    """Predict the output given the input features and parameters."""
    return np.round(sigmoid(np.dot(X, theta)))

def predict_one_point(hours_studied, theta):
    """Predict the outcome for a single point."""
    x = np.array([1, hours_studied])
    predicted_probability = sigmoid(np.dot(x, theta))
    predicted_class = 1 if predicted_probability >= 0.5 else 0
    return predicted_class

# Define the dataset
cup_shape = np.array([0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0])
class_counts = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])

# Add bias term to the features
X = np.column_stack((np.ones(len(cup_shape)), class_counts))

# Initialize parameters
theta = np.zeros(X.shape[1])  # Initialize weights and bias

# Set hyperparameters
learning_rate = 0.1
iterations = 1000

# Perform gradient descent
for _ in range(iterations):
    # Compute cost
    cost = compute_cost(X, class_counts ,theta)
    # Compute gradient
    gradient = compute_gradient(X, class_counts, theta)
    # Update parameters
    theta = update_parameters(theta, gradient, learning_rate)

# Predict the outcome for a single point (e.g., 2.5 hours studied)
cup_shape_to_predict = 2
predicted_class = predict_one_point(cup_shape_to_predict, theta)
print(f"The predicted outcome for {cup_shape_to_predict} hours studied is: {predicted_class}")

# Transform data using Box-Cox transformation for visualization
transformed_hours_studied, lambda_value = boxcox(cup_shape)

# Plot the original data points
plt.scatter(transformed_hours_studied, class_counts, color='blue', label='Original data')

# Plot the logistic curve
x_values = np.linspace(min(transformed_hours_studied), max(transformed_hours_studied), 100)
X_new = np.column_stack((np.ones(len(x_values)), x_values))
predictions = predict(X_new, theta)
plt.plot(x_values, predictions, color='red', label='Logistic curve')

plt.xlabel('Cap size Predickted (Box-Cox)')
plt.ylabel('Survey Result (0: In-edible, 1: Edible)')
plt.title('Logistic Regression with Box-Cox Transformed Data')
plt.legend()
plt.grid(True)
plt.show()

print(f'Optimal lambda value for Box-Cox transformation: {lambda_value}')

"""REPORT:Our dataset is from kaggle (mushroom cleaned). It has [54035 rows x 2 columns].We are determining whether the mushroom is edible or not based on several factors such as cap-diameter, cap-shape,	gill-attachment,	gill-color, stem-height, 	stem-width,stem-color,	season.In our implementation we have taken only the cap size as the independent variable and the class as the target variable. The class is Binary with 0 as Edible and 1 as Poisonous.
We have performed Pre-processing i.e, deleted null and duplicated records, our data is cleaned with no null or duplicated values. We have done Hyper-parameter Tuning with 70 % testing and 30% training data. We have accurracy of 100% becase of overfitting(LIMITATION : since we are considering only 1 independent variable column, the accuracy is 100% i.e overfitting issue SOLUTION: We can consider 2 or more independent column for an acuurate accuracy.)IF we consider more parameters we can get rid of this issue of overfitting.We have implemented and plotted  the correlation matrix that isused to identify and quantify the linear relationships between pairs of features.We  have performed Scaling used to standardize the range of features. We then perform the Logistic regression for binary classification tasks because it models the probability of a binary outcome using a logistic function, providing a clear probabilistic interpretation and efficient training.
"""